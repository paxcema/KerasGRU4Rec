{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M4-MovieLensBatchdserParallel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "bhMT2zYloMpm"
      },
      "cell_type": "markdown",
      "source": [
        "#**Proyecto - Sistemas Recomendadores - IIC3633**\n",
        "\n",
        "## Implementación en Keras de Session-Based RNNs for Recommendation con soft atenttion\n",
        "\n",
        "### V2: Implementación de embedding sobre one-hot vectors para entrenamiento más eficiente y modelo más chico"
      ]
    },
    {
      "metadata": {
        "id": "morbz0DMBo6M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Manejo de Google Drive\n",
        "!pip install -U -q PyDrive\n",
        "!pip install humanize\n",
        "!pip install GPUtil\n",
        "\n",
        "from google.colab import drive, auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3kJRW-qQ17_Q",
        "outputId": "77fcb5f9-aa88-4da2-dbce-4fb384613f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import psutil\n",
        "import humanize\n",
        "#import pyreclab\n",
        "import GPUtil as GPU\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.losses import cosine_proximity, categorical_crossentropy\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers.core import Permute, Reshape, RepeatVector\n",
        "from keras.layers import Input, Dense, Dropout, CuDNNGRU, Embedding, concatenate, Lambda, multiply, merge, Flatten\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nfW00EfwSNQ6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cargamos dataframes preprocesados de MovieLens20MM\n",
        "PATH_TO_ALL_TRAIN = './drive/My Drive/Cursos/2018/IIC3633/processedData/movie_all_train_tr.txt'\n",
        "PATH_TO_TRAIN = './drive/My Drive/Cursos/2018/IIC3633/processedData/movie_train_tr.txt'\n",
        "PATH_TO_DEV = './drive/My Drive/Cursos/2018/IIC3633/processedData/movie_train_valid.txt'\n",
        "PATH_TO_TEST = './drive/My Drive/Cursos/2018/IIC3633/processedData/movie_test.txt'\n",
        "\n",
        "train_data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId':np.int64})\n",
        "dev_data = pd.read_csv(PATH_TO_DEV, sep='\\t', dtype={'ItemId':np.int64})\n",
        "test_data = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QUXlHrRcBnNh",
        "colab_type": "code",
        "outputId": "bad16b9b-dc4a-414d-b715-615365531d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SessionId</th>\n",
              "      <th>ItemId</th>\n",
              "      <th>Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18</td>\n",
              "      <td>186</td>\n",
              "      <td>1267347706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>858</td>\n",
              "      <td>1236356241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18</td>\n",
              "      <td>912</td>\n",
              "      <td>1283426281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18</td>\n",
              "      <td>1221</td>\n",
              "      <td>1236356224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>1230</td>\n",
              "      <td>1236293194</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SessionId  ItemId        Time\n",
              "0         18     186  1267347706\n",
              "1         18     858  1236356241\n",
              "2         18     912  1283426281\n",
              "3         18    1221  1236356224\n",
              "4         18    1230  1236293194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "igwPqVSyBnNn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SessionDataset:\n",
        "    def __init__(self, data, sep='\\t', session_key='SessionId', item_key='ItemId', time_key='Time', n_samples=-1, itemmap=None, time_sort=False, sort = True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: path of the csv file\n",
        "            sep: separator for the csv\n",
        "            session_key, item_key, time_key: name of the fields corresponding to the sessions, items, time\n",
        "            n_samples: the number of samples to use. If -1, use the whole dataset.\n",
        "            itemmap: mapping between item IDs and item indices\n",
        "            time_sort: whether to sort the sessions by time or not\n",
        "        \"\"\"\n",
        "        self.df = data\n",
        "        self.session_key = session_key\n",
        "        self.item_key = item_key\n",
        "        self.time_key = time_key\n",
        "        self.time_sort = time_sort\n",
        "        self.add_item_indices(itemmap=itemmap)\n",
        "        if sort:\n",
        "          self.df.sort_values([session_key, time_key], inplace=True)\n",
        "          #print(self.df.head)\n",
        "        #self.add_item_indices(itemmap=itemmap)\n",
        "\n",
        "        #Sort the df by time, and then by session ID. That is, df is sorted by session ID and\n",
        "        #clicks within a session are next to each other, where the clicks within a session are time-ordered.\n",
        "\n",
        "        self.click_offsets = self.get_click_offsets()\n",
        "        self.session_idx_arr = self.order_session_idx()\n",
        "        \n",
        "        \n",
        "    def get_click_offsets(self):\n",
        "        \"\"\"\n",
        "        Return the offsets of the beginning clicks of each session IDs,\n",
        "        where the offset is calculated against the first click of the first session ID.\n",
        "        \"\"\"\n",
        "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype=np.int32)\n",
        "        # group & sort the df by session_key and get the offset values\n",
        "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
        "\n",
        "        return offsets\n",
        "    \n",
        "\n",
        "    def order_session_idx(self):\n",
        "        \"\"\" Order the session indices \"\"\"\n",
        "        if self.time_sort:\n",
        "            # starting time for each sessions, sorted by session IDs\n",
        "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
        "            # order the session indices by session starting times\n",
        "            session_idx_arr = np.argsort(sessions_start_time)\n",
        "        else:\n",
        "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
        "\n",
        "        return session_idx_arr\n",
        "    \n",
        "    \n",
        "    def add_item_indices(self, itemmap=None):\n",
        "        \"\"\" \n",
        "        Add item index column named \"item_idx\" to the df\n",
        "        Args:\n",
        "            itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
        "        \"\"\"\n",
        "        if itemmap is None:\n",
        "            item_ids = self.df[self.item_key].unique()  # unique item ids\n",
        "            item2idx = pd.Series(data=np.arange(len(item_ids)),\n",
        "                                 index=item_ids)\n",
        "            itemmap = pd.DataFrame({self.item_key:item_ids,\n",
        "                                   'item_idx':item2idx[item_ids].values})\n",
        "        \n",
        "        self.itemmap = itemmap\n",
        "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how='inner')\n",
        "        \n",
        "    \n",
        "    @property    \n",
        "    def items(self):\n",
        "        return self.itemmap.ItemId.unique()\n",
        "        \n",
        "\n",
        "class SessionDataLoader:\n",
        "    def __init__(self, dataset, batch_size=50):\n",
        "        \"\"\"\n",
        "        A class for creating session-parallel mini-batches.\n",
        "        Args:\n",
        "             dataset (SessionDataset): the session dataset to generate the batches from\n",
        "             batch_size (int): size of the batch\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
        "        Yields:\n",
        "            input (B,): torch.FloatTensor. Item indices that will be encoded as one-hot vectors later.\n",
        "            target (B,): a Variable that stores the target item indices\n",
        "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
        "        \"\"\"\n",
        "\n",
        "        # initializations\n",
        "        df = self.dataset.df\n",
        "        session_key='SessionId'\n",
        "        item_key='ItemId'\n",
        "        time_key='TimeStamp'\n",
        "        self.n_items = df[item_key].nunique()+1\n",
        "        click_offsets = self.dataset.click_offsets\n",
        "        session_idx_arr = self.dataset.session_idx_arr\n",
        "\n",
        "        iters = np.arange(self.batch_size)\n",
        "        maxiter = iters.max()\n",
        "        start = click_offsets[session_idx_arr[iters]]\n",
        "        end = click_offsets[session_idx_arr[iters] + 1]\n",
        "        mask = [] # indicator for the sessions to be terminated\n",
        "        finished = False        \n",
        "\n",
        "        while not finished:\n",
        "            minlen = (end - start).min()\n",
        "            # Item indices(for embedding) for clicks where the first sessions start\n",
        "            idx_target = df.item_idx.values[start]\n",
        "            for i in range(minlen - 1):\n",
        "                # Build inputs & targets\n",
        "                idx_input = idx_target\n",
        "                idx_target = df.item_idx.values[start + i + 1]\n",
        "                input = idx_input\n",
        "                target = idx_target\n",
        "                yield input, target, mask\n",
        "                \n",
        "            # click indices where a particular session meets second-to-last element\n",
        "            start = start + (minlen - 1)\n",
        "            # see if how many sessions should terminate\n",
        "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
        "            for idx in mask:\n",
        "                maxiter += 1\n",
        "                if maxiter >= len(click_offsets) - 1:\n",
        "                    finished = True\n",
        "                    break\n",
        "                # update the next starting/ending point\n",
        "                iters[idx] = maxiter\n",
        "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
        "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eQslL5CpRjdQ",
        "outputId": "0084c683-b0ac-494d-ab1c-ffaf81fdfd75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 512 #como en el paper\n",
        "session_max_len = 100\n",
        "embeddingp=False\n",
        "\n",
        "n_items = len(train_data['ItemId'].unique())+1\n",
        "print(\"Items unicos training:\", n_items)\n",
        "\n",
        "dev_n_items = len(dev_data['ItemId'].unique())+1\n",
        "print(\"Items unicos dev:\", dev_n_items)\n",
        "\n",
        "test_n_items = len(test_data['ItemId'].unique())+1\n",
        "print(\"Items unicos testing:\", test_n_items)\n",
        "\n",
        "train_samples_qty = len(train_data['SessionId'].unique()) # cantidad sesiones no augmentadas de train\n",
        "print(\"Sesiones training:\", train_samples_qty)\n",
        "\n",
        "dev_samples_qty = len(dev_data['SessionId'].unique()) # cantidad sesiones no augmentadas de dev\n",
        "print(\"Sesiones validation:\",dev_samples_qty)\n",
        "\n",
        "test_samples_qty = len(test_data['SessionId'].unique()) # cantidad sesiones no augmentadas de test\n",
        "print(\"Sesiones testing:\", test_samples_qty)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Items unicos training: 11619\n",
            "Items unicos dev: 10105\n",
            "Items unicos testing: 10366\n",
            "Sesiones training: 19853\n",
            "Sesiones validation: 5749\n",
            "Sesiones testing: 5271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1i_adI_ASgDi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_fraction = 1#256 # 1/fraction es la cantidad de sesiones mas recientes a considerar\n",
        "dev_fraction = 1#2\n",
        "\n",
        "train_offset_step=train_samples_qty//batch_size\n",
        "dev_offset_step=dev_samples_qty//batch_size\n",
        "test_offset_step=test_samples_qty//batch_size\n",
        "\n",
        "\n",
        "aux = [0]\n",
        "aux.extend(list(train_data['ItemId'].unique()))\n",
        "itemids = np.array(aux)\n",
        "itemidmap = pd.Series(data=np.arange(n_items), index=itemids) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O5_sa72xSF50",
        "outputId": "0be4421a-425c-4738-e092-096d2e06a534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        }
      },
      "cell_type": "code",
      "source": [
        "# Modelo\n",
        "\n",
        "# ToDo: self-attention\n",
        "\n",
        "def attention_3d_block(inputs, TIME_STEPS, SINGLE_ATTENTION_VECTOR=True):\n",
        "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
        "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
        "    if SINGLE_ATTENTION_VECTOR:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
        "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
        "    return output_attention_mul\n",
        "    \n",
        "emb_size = 50\n",
        "hidden_units = 100\n",
        "size = emb_size\n",
        "#size = emb_size if embeddingp else n_items\n",
        "\n",
        "#MODELO BASELINE + DROPOUT\n",
        "\n",
        "inputs = Input(batch_shape=(batch_size, 1, n_items))\n",
        "#emb = Embedding(n_items, emb_size, embeddings_initializer='uniform', input_length=session_max_len)(inputs)\n",
        "#drop1 = Dropout(0.25)(emb)\n",
        "gru, gru_states = CuDNNGRU(hidden_units, stateful=True, return_state=True)(inputs)# drop1) #\n",
        "drop2 = Dropout(0.25)(gru)\n",
        "#attention_mul = attention_3d_block(drop2, session_max_len)\n",
        "#attention_mul = Flatten()(attention_mul)\n",
        "predictions = Dense(n_items, activation='softmax')(drop2)#(attention_mul)#\n",
        "model = Model(input=inputs, output=[predictions])\n",
        "#custom_loss = custom_cosine_loss(itemidmap, n_items)\n",
        "# lr original es 0.0001\n",
        "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "# Try Nadam, too\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "#MODELO BASELINE + EMBEDDING + DROPOUT\n",
        "inputs2 = Input(shape=(n_items,), batch_shape = (batch_size,n_items))\n",
        "emb2 = Embedding(input_dim = n_items, output_dim = emb_size, embeddings_initializer='uniform')(inputs2)\n",
        "drop12 = Dropout(0.25)(emb2)\n",
        "gru12 = CuDNNGRU(100, stateful=True)(drop12)\n",
        "drop22 = Dropout(0.25)(gru12)\n",
        "predictions2 = Dense(n_items, activation='softmax')(drop22)\n",
        "model2 = Model(input=inputs2, output=[predictions2])\n",
        "opt2 = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model2.compile(loss=categorical_crossentropy, optimizer=opt2)\n",
        "model2.summary()\n",
        "#filepath='./bast/model_checkpoint'\n",
        "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
        "callbacks_list = []#[checkpoint]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (512, 1, 11619)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_9 (CuDNNGRU)       [(512, 100), (512, 100)]  3516300   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (512, 100)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (512, 11619)              1173519   \n",
            "=================================================================\n",
            "Total params: 4,689,819\n",
            "Trainable params: 4,689,819\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        (512, 11619)              0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (512, 11619, 50)          580950    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (512, 11619, 50)          0         \n",
            "_________________________________________________________________\n",
            "cu_dnngru_10 (CuDNNGRU)      (512, 100)                45600     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (512, 100)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (512, 11619)              1173519   \n",
            "=================================================================\n",
            "Total params: 1,800,069\n",
            "Trainable params: 1,800,069\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "VpSLc1SrBnOH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_states(model):\n",
        "    return [K.get_value(s) for s,_ in model.state_updates]\n",
        "\n",
        "def set_states(model, states):\n",
        "    for (d,_), s in zip(model.state_updates, states):\n",
        "        K.set_value(d, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "giTt1rELBnOL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "train_dataset = SessionDataset(train_data)\n",
        "\n",
        "#print(dataset.df.head())\n",
        "\n",
        "for epoch in range(3):\n",
        "    loader = SessionDataLoader(train_dataset, batch_size=batch_size)\n",
        "    for feat, target, mask in loader:\n",
        "                \n",
        "        input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
        "        input_oh = np.expand_dims(input_oh, axis=1)\n",
        "        \n",
        "        target_oh = to_categorical(target, num_classes=loader.n_items)\n",
        "        \n",
        "        tr_loss = model.train_on_batch(input_oh, target_oh)\n",
        "        \n",
        "        real_mask = np.ones((batch_size, 1))\n",
        "        for elt in mask:\n",
        "            real_mask[elt, :] = 0\n",
        "        \n",
        "        hidden_states = get_states(model)[0]#512,100    #get_states(model)[0]\n",
        "               \n",
        "        hidden_states = np.multiply(real_mask, hidden_states)\n",
        "        hidden_states = np.array(hidden_states, dtype=np.float32)\n",
        "        #hidden_states = np.expand_dims(hidden_states, axis=0)\n",
        "        \n",
        "        #set_states(model, hidden_states)\\):\n",
        "        #print(hidden_states.shape)\n",
        "        #K.set_value(model.layers[1].states, hidden_states)\n",
        "        model.layers[1].reset_states(hidden_states)\n",
        "\n",
        "        print(tr_loss)    \n",
        "            \n",
        "    #pass\n",
        "    #print(input, target, mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54ams4GLBnOQ",
        "colab_type": "code",
        "outputId": "ee4b2911-27d0-4390-81eb-de6c7b16d1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "cell_type": "code",
      "source": [
        "weights = model.layers[1].get_weights()[0]\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# RECALL @ 20\n",
        "recall_k = 20\n",
        "\n",
        "#nbrs = NearestNeighbors(n_neighbors=recall_k, algorithm='ball_tree').fit(weights)\n",
        "#distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
        "# Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "test_dataset = SessionDataset(test_data, itemmap = train_dataset.itemmap)\n",
        "test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "n = 0\n",
        "suma = 0\n",
        "suma_baseline = 0\n",
        "\n",
        "for feat, label, mask in test_generator:\n",
        "    input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
        "    input_oh = np.expand_dims(input_oh, axis=1)\n",
        "    \n",
        "    target_oh = to_categorical(label, num_classes=loader.n_items)\n",
        "    \n",
        "    pred = model.predict(input_oh, batch_size=batch_size)\n",
        "\n",
        "    if n%100 == 0:\n",
        "        try:\n",
        "            print(\"{}:{}\".format(n, suma/n))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    for row_idx in range(feat.shape[0]):\n",
        "        #baseline_pred = obj.recommend( str(test_batch[0][row_idx][-1]), 20 )\n",
        "        pred_row = pred[row_idx] # 37484, #.reshape(1, -1) # 50,\n",
        "        label_row = target_oh[row_idx]        #.reshape(1, -1) # 50,\n",
        "        #print(pred_row)\n",
        "        #print(label_row)\n",
        "        idx1 = pred_row.argsort()[-recall_k:][::-1]\n",
        "        idx2 = label_row.argsort()[-1:][::-1]\n",
        "\n",
        "        n += 1\n",
        "        #print(idx1)\n",
        "        #print(idx2)\n",
        "        if idx2[0] in idx1:\n",
        "            suma += 1\n",
        "\n",
        "        #if idx2[0] in baseline_pred:\n",
        "        #  suma_baseline += 1\n",
        "\n",
        "print(\"Recall@{} epoch {}: {}\".format(recall_k, epoch, suma/n))\n",
        "\n",
        "#print(\"Recall@{} baseline: {}\".format(recall_k, suma_baseline/n))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12800:0.254609375\n",
            "25600:0.246484375\n",
            "38400:0.24130208333333333\n",
            "51200:0.23837890625\n",
            "64000:0.23809375\n",
            "76800:0.23796875\n",
            "89600:0.23910714285714285\n",
            "102400:0.24033203125\n",
            "115200:0.24151909722222223\n",
            "128000:0.2429296875\n",
            "140800:0.24310369318181818\n",
            "153600:0.2426171875\n",
            "Recall@20 epoch 2: 0.24271844660194175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wt4zQQYiJoxS",
        "colab_type": "code",
        "outputId": "17472b0a-356e-4c09-dcf0-b5d2292b3dfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "cell_type": "code",
      "source": [
        "weights = model.layers[1].get_weights()[0]\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# MRR @ 20\n",
        "mrr_k = 20\n",
        "\n",
        "#nbrs = NearestNeighbors(n_neighbors=recall_k, algorithm='ball_tree').fit(weights)\n",
        "#distances, indices = nbrs.kneighbors(weights) # Vienen ya ordenados! # Shape (37484, 20)\n",
        "# Paso 3: Dado un vector embedding arbitrario, obtener el item más cercano a éste. Aplicarla sobre los 20 anteriores.\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "test_dataset = SessionDataset(test_data, itemmap = train_dataset.itemmap)\n",
        "test_generator = SessionDataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "n = 0\n",
        "suma = 0\n",
        "suma_baseline = 0\n",
        "\n",
        "for feat, label, mask in test_generator:\n",
        "    input_oh = to_categorical(feat, num_classes=loader.n_items) \n",
        "    input_oh = np.expand_dims(input_oh, axis=1)\n",
        "    #print(label)\n",
        "    target_oh = to_categorical(label, num_classes=loader.n_items)\n",
        "    \n",
        "    pred = model.predict(input_oh, batch_size=batch_size)\n",
        "\n",
        "    if n%100 == 0:\n",
        "        try:\n",
        "            print(\"{}:{}\".format(n, suma/n))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    for row_idx in range(feat.shape[0]):\n",
        "        #baseline_pred = obj.recommend( str(test_batch[0][row_idx][-1]), 20 )\n",
        "        pred_row = pred[row_idx] # 37484, #.reshape(1, -1) # 50,\n",
        "        label_row = target_oh[row_idx]        #.reshape(1, -1) # 50,\n",
        "\n",
        "        idx1 = pred_row.argsort()[-mrr_k:][::-1]\n",
        "        idx2 = label_row.argsort()[-1:][::-1]\n",
        "\n",
        "        n += 1\n",
        "        #print(idx1)\n",
        "        #print(idx2)\n",
        "        if idx2[0] in idx1:\n",
        "            suma += 1/int((np.where(idx1 == idx2[0])[0]+1))        \n",
        "        #print(suma)\n",
        "        #print(n)\n",
        " \n",
        "       \n",
        "        #if idx2[0] in baseline_pred:\n",
        "        #  suma_baseline += 1\n",
        "\n",
        "print(\"MRR@{} epoch {}: {}\".format(mrr_k, epoch, suma/n))\n",
        "\n",
        "#print(\"Recall@{} baseline: {}\".format(recall_k, suma_baseline/n))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12800:0.08033569431239306\n",
            "25600:0.07825079227624515\n",
            "38400:0.07604660234253038\n",
            "51200:0.07523685116753252\n",
            "64000:0.07503162113923621\n",
            "76800:0.07518242360743978\n",
            "89600:0.07528284135734004\n",
            "102400:0.07581448387456885\n",
            "115200:0.07640642249057486\n",
            "128000:0.07667791720009232\n",
            "140800:0.07644054686484039\n",
            "153600:0.0764714043625866\n",
            "MRR@20 epoch 2: 0.07655110124218388\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}